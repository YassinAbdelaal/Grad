{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5MmLFTxCERFX"
      },
      "id": "5MmLFTxCERFX",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ErUi9S1BETa2",
        "outputId": "5a7c1c21-451b-4022-97b4-2f572caa15e5"
      },
      "id": "ErUi9S1BETa2",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ccfc5fa-683a-4283-aeab-2176dad93119\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0ccfc5fa-683a-4283-aeab-2176dad93119\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RecordingData.zip to RecordingData (2).zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"RecordingData.zip\"\n",
        "extract_dir = \"eye_data_unzipped\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n"
      ],
      "metadata": {
        "id": "SS5z32yFEXBI"
      },
      "id": "SS5z32yFEXBI",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "c6d2b272",
      "metadata": {
        "id": "c6d2b272"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. Basic helpers\n",
        "# ============================================================\n",
        "def compute_version_vergence(df):\n",
        "    \"\"\"\n",
        "    Add version (VX, VY) and vergence (GX, GY) columns.\n",
        "    VX/VY  = (LX + RX) / 2\n",
        "    GX/GY  =  LX - RX\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"VX\"] = (df[\"LX\"] + df[\"RX\"]) / 2.0\n",
        "    df[\"VY\"] = (df[\"LY\"] + df[\"RY\"]) / 2.0\n",
        "    df[\"GX\"] = df[\"LX\"] - df[\"RX\"]\n",
        "    df[\"GY\"] = df[\"LY\"] - df[\"RY\"]\n",
        "    return df\n",
        "\n",
        "\n",
        "def detect_sampling_rate(df):\n",
        "    \"\"\"\n",
        "    Estimate sampling rate from the T column (assumed ms).\n",
        "    \"\"\"\n",
        "    diffs = np.diff(df[\"T\"].values.astype(float))\n",
        "    diffs = diffs[diffs > 0]\n",
        "    if len(diffs) == 0:\n",
        "        return 100.0\n",
        "    mean_interval = np.mean(diffs)\n",
        "    if mean_interval <= 0:\n",
        "        return 100.0\n",
        "    return float(np.round(1000.0 / mean_interval, 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "2cdef920",
      "metadata": {
        "id": "2cdef920"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 3. Dynamic dispersion state machine\n",
        "# ============================================================\n",
        "def _rms_noise(values):\n",
        "    \"\"\"\n",
        "    RMS of (values - mean(values)).\n",
        "    Used as signal noise estimate over the last 25 samples.\n",
        "    \"\"\"\n",
        "    if len(values) < 2:\n",
        "        return 0.0\n",
        "    v = np.array(values, dtype=float)\n",
        "    mu = v.mean()\n",
        "    return float(np.sqrt(np.mean((v - mu) ** 2)))\n",
        "\n",
        "\n",
        "def detect_states_dynamic(df,\n",
        "                          base_threshold_deg=0.5,\n",
        "                          noise_factor=2.5,\n",
        "                          min_fix_dur_ms=50.0):\n",
        "    \"\"\"\n",
        "    Dynamic dispersion threshold algorithm (paper style).\n",
        "\n",
        "    - Uses version signal as CH (horizontal) and CV (vertical).\n",
        "    - Instantaneous \"state\" internally:\n",
        "        * 'stable'  -> eventually becomes fixation/transient\n",
        "        * 'moving'  -> becomes saccade\n",
        "        * 'distortion' -> missing signal\n",
        "    - A stable segment is labeled:\n",
        "        * fixation  if duration >= min_fix_dur_ms\n",
        "        * transient otherwise\n",
        "    - A moving segment is labeled saccade.\n",
        "    - Distortion segments remain distortion.\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    if n == 0:\n",
        "        return []\n",
        "\n",
        "    # CH / CV = version\n",
        "    CH = ((df[\"LX\"] + df[\"RX\"]) / 2.0).values.astype(float)\n",
        "    CV = ((df[\"LY\"] + df[\"RY\"]) / 2.0).values.astype(float)\n",
        "    T = df[\"T\"].values.astype(float)\n",
        "\n",
        "    # approximate dt from median diff\n",
        "    diffs = np.diff(T)\n",
        "    diffs = diffs[diffs > 0]\n",
        "    dt_ms = np.median(diffs) if len(diffs) > 0 else 10.0\n",
        "    min_fix_samples = max(1, int(np.ceil(min_fix_dur_ms / dt_ms)))\n",
        "\n",
        "    events = []\n",
        "\n",
        "    current_state = None          # 'stable', 'moving', 'distortion'\n",
        "    state_start = 0\n",
        "    state_CH = []\n",
        "    state_CV = []\n",
        "    tail_CH = []\n",
        "    tail_CV = []\n",
        "\n",
        "    def close_state(end_idx):\n",
        "        nonlocal current_state, state_start, state_CH, state_CV, tail_CH, tail_CV\n",
        "        if current_state is None:\n",
        "            return\n",
        "        start_idx = state_start\n",
        "        if end_idx < start_idx:\n",
        "            return\n",
        "\n",
        "        start_time = T[start_idx]\n",
        "        end_time = T[end_idx]\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # label according to the paper rules\n",
        "        if current_state == \"distortion\":\n",
        "            label = \"distortion\"\n",
        "        elif current_state == \"stable\":\n",
        "            if (end_idx - start_idx + 1) >= min_fix_samples:\n",
        "                label = \"fixation\"\n",
        "            else:\n",
        "                label = \"transient\"\n",
        "        elif current_state == \"moving\":\n",
        "            label = \"saccade\"\n",
        "        else:\n",
        "            label = \"transient\"\n",
        "\n",
        "        events.append({\n",
        "            \"state\": label,\n",
        "            \"start_idx\": start_idx,\n",
        "            \"end_idx\": end_idx,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"duration\": duration,\n",
        "        })\n",
        "\n",
        "        # reset state buffers\n",
        "        current_state = None\n",
        "        state_start = end_idx + 1\n",
        "        state_CH = []\n",
        "        state_CV = []\n",
        "        tail_CH = []\n",
        "        tail_CV = []\n",
        "\n",
        "    for i in range(n):\n",
        "        lx = df[\"LX\"].iloc[i]\n",
        "        rx = df[\"RX\"].iloc[i]\n",
        "        ly = df[\"LY\"].iloc[i]\n",
        "        ry = df[\"RY\"].iloc[i]\n",
        "\n",
        "        # 1) Distortion: any signal missing\n",
        "        if np.isnan(lx) or np.isnan(rx) or np.isnan(ly) or np.isnan(ry):\n",
        "            new_state = \"distortion\"\n",
        "            if current_state is not None and current_state != new_state:\n",
        "                close_state(i - 1)\n",
        "            if current_state is None:\n",
        "                current_state = new_state\n",
        "                state_start = i\n",
        "            continue\n",
        "\n",
        "        x = CH[i]\n",
        "        y = CV[i]\n",
        "\n",
        "        # update buffers\n",
        "        tail_CH.append(x)\n",
        "        tail_CV.append(y)\n",
        "        state_CH.append(x)\n",
        "        state_CV.append(y)\n",
        "\n",
        "        if len(tail_CH) > 25:\n",
        "            tail_CH.pop(0)\n",
        "            tail_CV.pop(0)\n",
        "\n",
        "        # dynamic threshold = 0.5 deg + 2.5 * RMS noise\n",
        "        noise_x = _rms_noise(tail_CH)\n",
        "        noise_y = _rms_noise(tail_CV)\n",
        "        threshold = base_threshold_deg + noise_factor * max(noise_x, noise_y)\n",
        "\n",
        "        # distance from current state's mean\n",
        "        if len(state_CH) > 0:\n",
        "            mu_x = float(np.mean(state_CH))\n",
        "            mu_y = float(np.mean(state_CV))\n",
        "            dist = np.sqrt((x - mu_x) ** 2 + (y - mu_y) ** 2)\n",
        "        else:\n",
        "            dist = 0.0\n",
        "\n",
        "        stable = dist <= threshold\n",
        "        new_state = \"stable\" if stable else \"moving\"\n",
        "\n",
        "        if current_state is None:\n",
        "            current_state = new_state\n",
        "            state_start = i\n",
        "        elif current_state in [\"stable\", \"moving\"]:\n",
        "            if new_state != current_state:\n",
        "                # state change: close previous segment\n",
        "                close_state(i - 1)\n",
        "                current_state = new_state\n",
        "                state_start = i\n",
        "                state_CH = [x]\n",
        "                state_CV = [y]\n",
        "        elif current_state == \"distortion\":\n",
        "            if new_state != \"distortion\":\n",
        "                close_state(i - 1)\n",
        "                current_state = new_state\n",
        "                state_start = i\n",
        "                state_CH = [x]\n",
        "                state_CV = [y]\n",
        "\n",
        "    # close last state\n",
        "    close_state(n - 1)\n",
        "\n",
        "    return events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "59f58b9b",
      "metadata": {
        "id": "59f58b9b"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. From states → fixations + saccades with direction\n",
        "# ============================================================\n",
        "def label_fixations_saccades(events, df):\n",
        "    \"\"\"\n",
        "    - Keep only fixation and saccade events.\n",
        "    - For saccades: direction = progressive (ΔVX > 0) or regressive (ΔVX <= 0).\n",
        "    - For fixations: direction = direction of preceding saccade\n",
        "      (as described in the paper).\n",
        "    \"\"\"\n",
        "    VX = ((df[\"LX\"] + df[\"RX\"]) / 2.0).values.astype(float)\n",
        "\n",
        "    # step 1: give directions to saccades\n",
        "    for ev in events:\n",
        "        if ev[\"state\"] == \"saccade\":\n",
        "            s = ev[\"start_idx\"]\n",
        "            e = ev[\"end_idx\"]\n",
        "            if e <= s:\n",
        "                ev[\"direction\"] = \"unknown\"\n",
        "            else:\n",
        "                delta = VX[e] - VX[s]\n",
        "                ev[\"direction\"] = \"progressive\" if delta > 0 else \"regressive\"\n",
        "        else:\n",
        "            ev[\"direction\"] = \"unknown\"\n",
        "\n",
        "    # step 2: propagate direction to following fixations\n",
        "    last_dir = \"unknown\"\n",
        "    for ev in events:\n",
        "        if ev[\"state\"] == \"saccade\":\n",
        "            last_dir = ev[\"direction\"]\n",
        "        elif ev[\"state\"] == \"fixation\":\n",
        "            ev[\"direction\"] = last_dir\n",
        "\n",
        "    # keep only fixations & saccades\n",
        "    fs = [ev for ev in events if ev[\"state\"] in [\"fixation\", \"saccade\"]]\n",
        "    return fs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "71cabe70",
      "metadata": {
        "id": "71cabe70"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. Per-event feature computation\n",
        "# ============================================================\n",
        "def compute_single_event_features(df, ev):\n",
        "    \"\"\"\n",
        "    Paper's 6 parameter types, each measured for version x/y and vergence x/y:\n",
        "\n",
        "        (1) duration\n",
        "        (2) distance spanning the event        → version_x/y_distance, vergence_x/y_distance\n",
        "        (3) average eye position               → version_x/y_mean,     vergence_x/y_mean\n",
        "        (4) std of position                    → version_x/y_std,      vergence_x/y_std\n",
        "        (5) max range between any two positions → version_x/y_range,    vergence_x/y_range\n",
        "        (6) accumulated distance               → version_x/y_accum,    vergence_x/y_accum\n",
        "\n",
        "    Total: 1 + (5 × 4) = 21 parameters per event\n",
        "    \"\"\"\n",
        "    s, e = ev[\"start_idx\"], ev[\"end_idx\"]\n",
        "    seg  = df.iloc[s:e+1]\n",
        "\n",
        "    VX = seg[\"VX\"].values.astype(float)\n",
        "    VY = seg[\"VY\"].values.astype(float)\n",
        "    GX = seg[\"GX\"].values.astype(float)\n",
        "    GY = seg[\"GY\"].values.astype(float)\n",
        "\n",
        "    def accum(arr):\n",
        "        \"\"\"Sum of absolute differences (1D path length).\"\"\"\n",
        "        return float(np.sum(np.abs(np.diff(arr)))) if len(arr) > 1 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"event_type\": ev[\"state\"],\n",
        "        \"direction\":  ev[\"direction\"],\n",
        "\n",
        "        # (1) duration\n",
        "        \"duration\":                  float(ev[\"duration\"]),\n",
        "\n",
        "        # (2) distance spanning the event (end - start, signed)\n",
        "        \"version_x_distance\":       float(VX[-1] - VX[0]),\n",
        "        \"version_y_distance\":       float(VY[-1] - VY[0]),\n",
        "        \"vergence_x_distance\":      float(GX[-1] - GX[0]),\n",
        "        \"vergence_y_distance\":      float(GY[-1] - GY[0]),\n",
        "\n",
        "        # (3) average position\n",
        "        \"version_x_mean\":           float(VX.mean()),\n",
        "        \"version_y_mean\":           float(VY.mean()),\n",
        "        \"vergence_x_mean\":          float(GX.mean()),\n",
        "        \"vergence_y_mean\":          float(GY.mean()),\n",
        "\n",
        "        # (4) std of position\n",
        "        \"version_x_std\":            float(VX.std(ddof=0)),\n",
        "        \"version_y_std\":            float(VY.std(ddof=0)),\n",
        "        \"vergence_x_std\":           float(GX.std(ddof=0)),\n",
        "        \"vergence_y_std\":           float(GY.std(ddof=0)),\n",
        "\n",
        "        # (5) max range (max - min)\n",
        "        \"version_x_range\":          float(VX.max() - VX.min()),\n",
        "        \"version_y_range\":          float(VY.max() - VY.min()),\n",
        "        \"vergence_x_range\":         float(GX.max() - GX.min()),\n",
        "        \"vergence_y_range\":         float(GY.max() - GY.min()),\n",
        "\n",
        "        # (6) accumulated distance (1D path length per axis)\n",
        "        \"version_x_accum\":          accum(VX),\n",
        "        \"version_y_accum\":          accum(VY),\n",
        "        \"vergence_x_accum\":         accum(GX),\n",
        "        \"vergence_y_accum\":         accum(GY),\n",
        "    }\n",
        "\n",
        "\n",
        "def build_event_dataframe(df, events_fs):\n",
        "    \"\"\"Turn fixation/saccade event list into a per-event DataFrame.\"\"\"\n",
        "    rows = [compute_single_event_features(df, ev) for ev in events_fs]\n",
        "    if len(rows) == 0:\n",
        "        return pd.DataFrame()\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "15d3e2a2",
      "metadata": {
        "id": "15d3e2a2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. Summarize to exactly 168 features (subject-level)\n",
        "# ============================================================\n",
        "def summarize_168_features(events_df):\n",
        "    \"\"\"\n",
        "    4 event types × 21 parameters × 2 stats (mean, std) = 168 features\n",
        "\n",
        "    Event types:\n",
        "        fix_prog  : fixations after progressive saccades\n",
        "        fix_reg   : fixations after regressive saccades\n",
        "        sac_prog  : progressive saccades\n",
        "        sac_reg   : regressive saccades\n",
        "\n",
        "    21 parameters per event:\n",
        "        duration (1)\n",
        "        + distance, mean, std, range, accum × version x/y & vergence x/y (5 × 4 = 20)\n",
        "    \"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    groups = {\n",
        "        \"fix_prog\": (events_df[\"event_type\"] == \"fixation\") & (events_df[\"direction\"] == \"progressive\"),\n",
        "        \"fix_reg\":  (events_df[\"event_type\"] == \"fixation\") & (events_df[\"direction\"] == \"regressive\"),\n",
        "        \"sac_prog\": (events_df[\"event_type\"] == \"saccade\")  & (events_df[\"direction\"] == \"progressive\"),\n",
        "        \"sac_reg\":  (events_df[\"event_type\"] == \"saccade\")  & (events_df[\"direction\"] == \"regressive\"),\n",
        "    }\n",
        "\n",
        "    # 21 parameters — must match keys in compute_single_event_features\n",
        "    feature_cols = [\n",
        "        \"duration\",\n",
        "        \"version_x_distance\",  \"version_y_distance\",  \"vergence_x_distance\",  \"vergence_y_distance\",\n",
        "        \"version_x_mean\",      \"version_y_mean\",      \"vergence_x_mean\",      \"vergence_y_mean\",\n",
        "        \"version_x_std\",       \"version_y_std\",       \"vergence_x_std\",       \"vergence_y_std\",\n",
        "        \"version_x_range\",     \"version_y_range\",     \"vergence_x_range\",     \"vergence_y_range\",\n",
        "        \"version_x_accum\",     \"version_y_accum\",     \"vergence_x_accum\",     \"vergence_y_accum\",\n",
        "    ]\n",
        "\n",
        "    for gname, mask in groups.items():\n",
        "        g = events_df[mask]\n",
        "\n",
        "        if len(g) == 0:\n",
        "            for col in feature_cols:\n",
        "                summary[f\"{gname}_{col}_mean\"] = 0.0\n",
        "                summary[f\"{gname}_{col}_std\"]  = 0.0\n",
        "        else:\n",
        "            for col in feature_cols:\n",
        "                vals = g[col].values.astype(float)\n",
        "                summary[f\"{gname}_{col}_mean\"] = float(np.mean(vals))\n",
        "                summary[f\"{gname}_{col}_std\"]  = float(np.std(vals, ddof=0))\n",
        "\n",
        "    return pd.Series(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "93b360a6",
      "metadata": {
        "id": "93b360a6",
        "outputId": "9d1c867e-201c-4215-a561-53819ebb25f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping eye_data_unzipped/Recording Data/Subject codes.txt — missing required columns.\n",
            "Processed eye_data_unzipped/Recording Data/414FJ1/A1R.txt: fix=71, sac=88\n",
            "Processed eye_data_unzipped/Recording Data/141NH4/A1R.txt: fix=47, sac=89\n",
            "Processed eye_data_unzipped/Recording Data/512PM4/A1R.txt: fix=52, sac=135\n",
            "Processed eye_data_unzipped/Recording Data/721AE4/A1R.txt: fix=31, sac=124\n",
            "Processed eye_data_unzipped/Recording Data/132IM3/A1R.txt: fix=38, sac=86\n",
            "Processed eye_data_unzipped/Recording Data/512ED1/A1R.txt: fix=73, sac=116\n",
            "Processed eye_data_unzipped/Recording Data/622FD1/A1R.txt: fix=66, sac=196\n",
            "Processed eye_data_unzipped/Recording Data/721AF1/A1R.txt: fix=77, sac=132\n",
            "Processed eye_data_unzipped/Recording Data/712WA1/A1R.txt: fix=78, sac=105\n",
            "Processed eye_data_unzipped/Recording Data/132SJ1/A1R.txt: fix=64, sac=120\n",
            "Processed eye_data_unzipped/Recording Data/512LM1/A1R.txt: fix=82, sac=111\n",
            "Processed eye_data_unzipped/Recording Data/796PA3/A1R.txt: fix=55, sac=112\n",
            "Processed eye_data_unzipped/Recording Data/754TD3/A1R.txt: fix=63, sac=101\n",
            "Processed eye_data_unzipped/Recording Data/133GJ3/A1R.txt: fix=55, sac=149\n",
            "Processed eye_data_unzipped/Recording Data/112KA1/A1R.txt: fix=76, sac=118\n",
            "Processed eye_data_unzipped/Recording Data/762SL1/A1R.txt: fix=73, sac=154\n",
            "Processed eye_data_unzipped/Recording Data/423MD1/A1R.txt: fix=68, sac=91\n",
            "Processed eye_data_unzipped/Recording Data/794EM1/A1R.txt: fix=81, sac=121\n",
            "Processed eye_data_unzipped/Recording Data/131SA2/A1R.txt: fix=69, sac=169\n",
            "Processed eye_data_unzipped/Recording Data/414JM3/A1R.txt: fix=52, sac=101\n",
            "Processed eye_data_unzipped/Recording Data/522BK1/A1R.txt: fix=80, sac=153\n",
            "Processed eye_data_unzipped/Recording Data/343SF1/A1R.txt: fix=64, sac=107\n",
            "Processed eye_data_unzipped/Recording Data/322EM1/A1R.txt: fix=81, sac=125\n",
            "Processed eye_data_unzipped/Recording Data/512EO3/A1R.txt: fix=57, sac=109\n",
            "Processed eye_data_unzipped/Recording Data/211CF3/A1R.txt: fix=50, sac=137\n",
            "Processed eye_data_unzipped/Recording Data/755TD3/A1R.txt: fix=58, sac=203\n",
            "Processed eye_data_unzipped/Recording Data/332PM1/A1R.txt: fix=77, sac=167\n",
            "Processed eye_data_unzipped/Recording Data/112JU3/A1R.txt: fix=68, sac=148\n",
            "Processed eye_data_unzipped/Recording Data/775DA3/A1R.txt: fix=43, sac=119\n",
            "Processed eye_data_unzipped/Recording Data/133AM4/A1R.txt: fix=34, sac=149\n",
            "Processed eye_data_unzipped/Recording Data/772CC1/A1R.txt: fix=60, sac=63\n",
            "Processed eye_data_unzipped/Recording Data/753PJ3/A1R.txt: fix=41, sac=61\n",
            "Processed eye_data_unzipped/Recording Data/523GM1/A1R.txt: fix=64, sac=83\n",
            "Processed eye_data_unzipped/Recording Data/143SM3/A1R.txt: fix=75, sac=126\n",
            "Processed eye_data_unzipped/Recording Data/721KJ2/A1R.txt: fix=65, sac=170\n",
            "Processed eye_data_unzipped/Recording Data/322RE1/A1R.txt: fix=67, sac=184\n",
            "Processed eye_data_unzipped/Recording Data/422BC1/A1R.txt: fix=82, sac=103\n",
            "Processed eye_data_unzipped/Recording Data/826ES4/A1R.txt: fix=81, sac=114\n",
            "Processed eye_data_unzipped/Recording Data/712FJ1/A1R.txt: fix=73, sac=147\n",
            "Processed eye_data_unzipped/Recording Data/312BM4/A1R.txt: fix=51, sac=109\n",
            "Processed eye_data_unzipped/Recording Data/794SD1/A1R.txt: fix=80, sac=137\n",
            "Processed eye_data_unzipped/Recording Data/423MM3/A1R.txt: fix=42, sac=115\n",
            "Processed eye_data_unzipped/Recording Data/532EJ2/A1R.txt: fix=67, sac=153\n",
            "Processed eye_data_unzipped/Recording Data/745RK3/A1R.txt: fix=53, sac=113\n",
            "Processed eye_data_unzipped/Recording Data/143JM3/A1R.txt: fix=49, sac=161\n",
            "Processed eye_data_unzipped/Recording Data/751AH1/A1R.txt: fix=66, sac=138\n",
            "Processed eye_data_unzipped/Recording Data/511RD3/A1R.txt: fix=55, sac=151\n",
            "Processed eye_data_unzipped/Recording Data/344LL1/A1R.txt: fix=74, sac=106\n",
            "Processed eye_data_unzipped/Recording Data/724VS1/A1R.txt: fix=63, sac=90\n",
            "Processed eye_data_unzipped/Recording Data/721SM3/A1R.txt: fix=78, sac=116\n",
            "Processed eye_data_unzipped/Recording Data/322PS3/A1R.txt: fix=51, sac=122\n",
            "Processed eye_data_unzipped/Recording Data/211LJ1/A1R.txt: fix=75, sac=121\n",
            "Processed eye_data_unzipped/Recording Data/622LB3/A1R.txt: fix=58, sac=127\n",
            "Processed eye_data_unzipped/Recording Data/723SJ1/A1R.txt: fix=76, sac=110\n",
            "Processed eye_data_unzipped/Recording Data/712HL3/A1R.txt: fix=51, sac=113\n",
            "Processed eye_data_unzipped/Recording Data/613SP3/A1R.txt: fix=51, sac=119\n",
            "Processed eye_data_unzipped/Recording Data/762LB3/A1R.txt: fix=62, sac=156\n",
            "Processed eye_data_unzipped/Recording Data/742EO3/A1R.txt: fix=58, sac=146\n",
            "Processed eye_data_unzipped/Recording Data/132FJ1/A1R.txt: fix=71, sac=141\n",
            "Processed eye_data_unzipped/Recording Data/622RM1/A1R.txt: fix=77, sac=108\n",
            "Processed eye_data_unzipped/Recording Data/826SJ1/A1R.txt: fix=67, sac=138\n",
            "Processed eye_data_unzipped/Recording Data/613KF1/A1R.txt: fix=68, sac=139\n",
            "Processed eye_data_unzipped/Recording Data/714GC4/A1R.txt: fix=68, sac=110\n",
            "Processed eye_data_unzipped/Recording Data/623JA1/A1R.txt: fix=63, sac=228\n",
            "Processed eye_data_unzipped/Recording Data/752LM3/A1R.txt: fix=39, sac=98\n",
            "Processed eye_data_unzipped/Recording Data/754WF3/A1R.txt: fix=51, sac=107\n",
            "Processed eye_data_unzipped/Recording Data/794LA3/A1R.txt: fix=44, sac=147\n",
            "Processed eye_data_unzipped/Recording Data/312BE1/A1R.txt: fix=51, sac=84\n",
            "Processed eye_data_unzipped/Recording Data/762VH1/A1R.txt: fix=74, sac=155\n",
            "Processed eye_data_unzipped/Recording Data/772AK3/A1R.txt: fix=30, sac=51\n",
            "Processed eye_data_unzipped/Recording Data/723DT1/A1R.txt: fix=72, sac=88\n",
            "Processed eye_data_unzipped/Recording Data/421SP4/A1R.txt: fix=47, sac=98\n",
            "Processed eye_data_unzipped/Recording Data/793MD1/A1R.txt: fix=76, sac=105\n",
            "Processed eye_data_unzipped/Recording Data/621NL4/A1R.txt: fix=69, sac=130\n",
            "Processed eye_data_unzipped/Recording Data/421LE2/A1R.txt: fix=70, sac=109\n",
            "Processed eye_data_unzipped/Recording Data/761EL1/A1R.txt: fix=62, sac=114\n",
            "Processed eye_data_unzipped/Recording Data/726HD1/A1R.txt: fix=67, sac=99\n",
            "Processed eye_data_unzipped/Recording Data/421AT2/A1R.txt: fix=71, sac=159\n",
            "Processed eye_data_unzipped/Recording Data/726PT3/A1R.txt: fix=35, sac=93\n",
            "Processed eye_data_unzipped/Recording Data/741US2/A1R.txt: fix=60, sac=127\n",
            "Processed eye_data_unzipped/Recording Data/781SD3/A1R.txt: fix=39, sac=138\n",
            "Processed eye_data_unzipped/Recording Data/713VC3/A1R.txt: fix=51, sac=196\n",
            "Processed eye_data_unzipped/Recording Data/721JL4/A1R.txt: fix=51, sac=116\n",
            "Processed eye_data_unzipped/Recording Data/775SM1/A1R.txt: fix=68, sac=168\n",
            "Processed eye_data_unzipped/Recording Data/311HL2/A1R.txt: fix=80, sac=114\n",
            "Processed eye_data_unzipped/Recording Data/761SJ1/A1R.txt: fix=61, sac=164\n",
            "Processed eye_data_unzipped/Recording Data/796NT3/A1R.txt: fix=51, sac=143\n",
            "Processed eye_data_unzipped/Recording Data/533OA3/A1R.txt: fix=34, sac=60\n",
            "Processed eye_data_unzipped/Recording Data/713PE4/A1R.txt: fix=64, sac=127\n",
            "Processed eye_data_unzipped/Recording Data/211ND1/A1R.txt: fix=62, sac=113\n",
            "Processed eye_data_unzipped/Recording Data/782SC3/A1R.txt: fix=54, sac=179\n",
            "Processed eye_data_unzipped/Recording Data/141BE2/A1R.txt: fix=66, sac=202\n",
            "Processed eye_data_unzipped/Recording Data/712KO3/A1R.txt: fix=51, sac=168\n",
            "Processed eye_data_unzipped/Recording Data/622SA3/A1R.txt: fix=52, sac=204\n",
            "Processed eye_data_unzipped/Recording Data/713PE2/A1R.txt: fix=78, sac=113\n",
            "Processed eye_data_unzipped/Recording Data/342GD1/A1R.txt: fix=82, sac=107\n",
            "Processed eye_data_unzipped/Recording Data/534SR1/A1R.txt: fix=61, sac=218\n",
            "Processed eye_data_unzipped/Recording Data/753HP1/A1R.txt: fix=51, sac=69\n",
            "Processed eye_data_unzipped/Recording Data/322BJ3/A1R.txt: fix=48, sac=127\n",
            "Processed eye_data_unzipped/Recording Data/753EM1/A1R.txt: fix=44, sac=103\n",
            "Processed eye_data_unzipped/Recording Data/111GM3/A1R.txt: fix=51, sac=140\n",
            "Processed eye_data_unzipped/Recording Data/826PP1/A1R.txt: fix=75, sac=162\n",
            "Processed eye_data_unzipped/Recording Data/522NC1/A1R.txt: fix=72, sac=122\n",
            "Processed eye_data_unzipped/Recording Data/782PR1/A1R.txt: fix=80, sac=183\n",
            "Processed eye_data_unzipped/Recording Data/745CI4/A1R.txt: fix=41, sac=91\n",
            "Processed eye_data_unzipped/Recording Data/623WL2/A1R.txt: fix=76, sac=118\n",
            "Processed eye_data_unzipped/Recording Data/793LL3/A1R.txt: fix=39, sac=55\n",
            "Processed eye_data_unzipped/Recording Data/612LJ3/A1R.txt: fix=55, sac=114\n",
            "Processed eye_data_unzipped/Recording Data/623GH1/A1R.txt: fix=57, sac=74\n",
            "Processed eye_data_unzipped/Recording Data/724SM3/A1R.txt: fix=35, sac=56\n",
            "Processed eye_data_unzipped/Recording Data/422JN3/A1R.txt: fix=56, sac=129\n",
            "Processed eye_data_unzipped/Recording Data/312HJ3/A1R.txt: fix=78, sac=180\n",
            "Processed eye_data_unzipped/Recording Data/752PJ3/A1R.txt: fix=60, sac=75\n",
            "Processed eye_data_unzipped/Recording Data/745DJ1/A1R.txt: fix=72, sac=131\n",
            "Processed eye_data_unzipped/Recording Data/342HM1/A1R.txt: fix=70, sac=84\n",
            "Processed eye_data_unzipped/Recording Data/534FT1/A1R.txt: fix=57, sac=76\n",
            "Processed eye_data_unzipped/Recording Data/142MM1/A1R.txt: fix=68, sac=132\n",
            "Processed eye_data_unzipped/Recording Data/141NK2/A1R.txt: fix=71, sac=130\n",
            "Processed eye_data_unzipped/Recording Data/343BL1/A1R.txt: fix=77, sac=113\n",
            "Processed eye_data_unzipped/Recording Data/511EE1/A1R.txt: fix=80, sac=118\n",
            "Processed eye_data_unzipped/Recording Data/761SJ3/A1R.txt: fix=44, sac=107\n",
            "Processed eye_data_unzipped/Recording Data/774HJ2/A1R.txt: fix=68, sac=124\n",
            "Processed eye_data_unzipped/Recording Data/142EJ1/A1R.txt: fix=53, sac=102\n",
            "Processed eye_data_unzipped/Recording Data/332JH3/A1R.txt: fix=52, sac=144\n",
            "Processed eye_data_unzipped/Recording Data/781PT1/A1R.txt: fix=78, sac=111\n",
            "Processed eye_data_unzipped/Recording Data/343SJ1/A1R.txt: fix=80, sac=133\n",
            "Processed eye_data_unzipped/Recording Data/512BM2/A1R.txt: fix=61, sac=136\n",
            "Processed eye_data_unzipped/Recording Data/781SE4/A1R.txt: fix=53, sac=73\n",
            "Processed eye_data_unzipped/Recording Data/522LA3/A1R.txt: fix=42, sac=100\n",
            "Processed eye_data_unzipped/Recording Data/141MS4/A1R.txt: fix=60, sac=136\n",
            "Processed eye_data_unzipped/Recording Data/351LR1/A1R.txt: fix=89, sac=152\n",
            "Processed eye_data_unzipped/Recording Data/351SS3/A1R.txt: fix=60, sac=139\n",
            "Processed eye_data_unzipped/Recording Data/795SJ1/A1R.txt: fix=71, sac=125\n",
            "Processed eye_data_unzipped/Recording Data/756FJ3/A1R.txt: fix=46, sac=87\n",
            "Processed eye_data_unzipped/Recording Data/725MF1/A1R.txt: fix=77, sac=103\n",
            "Processed eye_data_unzipped/Recording Data/523FM3/A1R.txt: fix=75, sac=150\n",
            "Processed eye_data_unzipped/Recording Data/533KM1/A1R.txt: fix=83, sac=118\n",
            "Processed eye_data_unzipped/Recording Data/346KU1/A1R.txt: fix=70, sac=199\n",
            "Processed eye_data_unzipped/Recording Data/751GJ3/A1R.txt: fix=31, sac=120\n",
            "Processed eye_data_unzipped/Recording Data/795MM1/A1R.txt: fix=78, sac=116\n",
            "Processed eye_data_unzipped/Recording Data/752NA1/A1R.txt: fix=71, sac=73\n",
            "Processed eye_data_unzipped/Recording Data/726OG1/A1R.txt: fix=76, sac=141\n",
            "Processed eye_data_unzipped/Recording Data/831OK2/A1R.txt: fix=68, sac=155\n",
            "Processed eye_data_unzipped/Recording Data/623BM4/A1R.txt: fix=39, sac=179\n",
            "Processed eye_data_unzipped/Recording Data/831PA3/A1R.txt: fix=52, sac=108\n",
            "Processed eye_data_unzipped/Recording Data/125KM1/A1R.txt: fix=60, sac=102\n",
            "Processed eye_data_unzipped/Recording Data/712SJ1/A1R.txt: fix=82, sac=137\n",
            "Processed eye_data_unzipped/Recording Data/701AA4/A1R.txt: fix=52, sac=156\n",
            "Processed eye_data_unzipped/Recording Data/701LV2/A1R.txt: fix=64, sac=162\n",
            "Processed eye_data_unzipped/Recording Data/224CM2/A1R.txt: fix=64, sac=148\n",
            "Processed eye_data_unzipped/Recording Data/522HD3/A1R.txt: fix=53, sac=96\n",
            "Processed eye_data_unzipped/Recording Data/111JA2/A1R.txt: fix=64, sac=165\n",
            "Processed eye_data_unzipped/Recording Data/825PA3/A1R.txt: fix=45, sac=100\n",
            "Processed eye_data_unzipped/Recording Data/796GA3/A1R.txt: fix=52, sac=129\n",
            "Processed eye_data_unzipped/Recording Data/343BJ3/A1R.txt: fix=48, sac=116\n",
            "Processed eye_data_unzipped/Recording Data/621NP2/A1R.txt: fix=73, sac=110\n",
            "Processed eye_data_unzipped/Recording Data/826AL2/A1R.txt: fix=82, sac=108\n",
            "Processed eye_data_unzipped/Recording Data/335NJ1/A1R.txt: fix=72, sac=91\n",
            "Processed eye_data_unzipped/Recording Data/791LK3/A1R.txt: fix=56, sac=120\n",
            "Processed eye_data_unzipped/Recording Data/762ET3/A1R.txt: fix=51, sac=92\n",
            "Processed eye_data_unzipped/Recording Data/711TA3/A1R.txt: fix=42, sac=110\n",
            "Processed eye_data_unzipped/Recording Data/352KM1/A1R.txt: fix=84, sac=108\n",
            "Processed eye_data_unzipped/Recording Data/752AK1/A1R.txt: fix=72, sac=121\n",
            "Processed eye_data_unzipped/Recording Data/724OP3/A1R.txt: fix=63, sac=212\n",
            "Processed eye_data_unzipped/Recording Data/533HM2/A1R.txt: fix=77, sac=101\n",
            "Processed eye_data_unzipped/Recording Data/335HJ3/A1R.txt: fix=55, sac=108\n",
            "Processed eye_data_unzipped/Recording Data/131CV4/A1R.txt: fix=47, sac=113\n",
            "Processed eye_data_unzipped/Recording Data/421LS4/A1R.txt: fix=47, sac=178\n",
            "Processed eye_data_unzipped/Recording Data/111RP1/A1R.txt: fix=76, sac=172\n",
            "Processed eye_data_unzipped/Recording Data/771TJ1/A1R.txt: fix=72, sac=139\n",
            "Processed eye_data_unzipped/Recording Data/732KD1/A1R.txt: fix=80, sac=200\n",
            "Processed eye_data_unzipped/Recording Data/742AJ3/A1R.txt: fix=37, sac=162\n",
            "Processed eye_data_unzipped/Recording Data/142SJ3/A1R.txt: fix=53, sac=138\n",
            "Processed eye_data_unzipped/Recording Data/795JM1/A1R.txt: fix=63, sac=204\n",
            "Processed eye_data_unzipped/Recording Data/612SA1/A1R.txt: fix=72, sac=124\n",
            "Processed eye_data_unzipped/Recording Data/761NM3/A1R.txt: fix=78, sac=137\n",
            "Processed eye_data_unzipped/Recording Data/142OJ3/A1R.txt: fix=55, sac=121\n",
            "Processed eye_data_unzipped/Recording Data/751GS1/A1R.txt: fix=70, sac=88\n",
            "Processed eye_data_unzipped/Recording Data/794GO3/A1R.txt: fix=51, sac=125\n",
            "Processed eye_data_unzipped/Recording Data/794HH3/A1R.txt: fix=47, sac=171\n",
            "Processed eye_data_unzipped/Recording Data/721HL2/A1R.txt: fix=61, sac=196\n",
            "Processed eye_data_unzipped/Recording Data/132AD3/A1R.txt: fix=67, sac=169\n",
            "Processed eye_data_unzipped/Recording Data/511YD3/A1R.txt: fix=32, sac=81\n",
            "Processed eye_data_unzipped/Recording Data/224BE4/A1R.txt: fix=53, sac=158\n",
            "Processed eye_data_unzipped/Recording Data/725OM3/A1R.txt: fix=56, sac=159\n",
            "\n",
            "✅ DONE. Saved → eye_features_168.csv\n"
          ]
        }
      ],
      "source": [
        "all_features = []\n",
        "\n",
        "for file in glob.glob(os.path.join(extract_dir, \"**/*.txt\"), recursive=True):\n",
        "    base = os.path.basename(file)\n",
        "    if base.lower().startswith(\"info\"):\n",
        "        continue  # skip meta files\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file, sep=\"\\t\", decimal=\",\", engine=\"python\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read {file}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Required columns\n",
        "    required_cols = {\"T\", \"LX\", \"LY\", \"RX\", \"RY\"}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        print(f\"⚠️ Skipping {file} — missing required columns.\")\n",
        "        continue\n",
        "\n",
        "    # Ensure numeric\n",
        "    for c in required_cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    # Sort by time\n",
        "    df = df.sort_values(\"T\").reset_index(drop=True)\n",
        "\n",
        "    # Compute version/vergence\n",
        "    df = compute_version_vergence(df)\n",
        "\n",
        "    # Sampling rate\n",
        "    sampling_rate = detect_sampling_rate(df)\n",
        "\n",
        "    # Detect fixations/saccades\n",
        "    states = detect_states_dynamic(df)\n",
        "    fs_events = label_fixations_saccades(states, df)\n",
        "\n",
        "    # Compute per-event features including vergence spans/distances\n",
        "    events_df = build_event_dataframe(df, fs_events)\n",
        "    if len(events_df) == 0:\n",
        "        print(f\"⚠️ No events found in {file}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Summarize to 168 subject-level features\n",
        "    feat = summarize_168_features(events_df)\n",
        "\n",
        "    # Add metadata\n",
        "    feat[\"file\"] = file\n",
        "    feat[\"sampling_rate\"] = sampling_rate\n",
        "    feat[\"n_fixations\"] = int((events_df[\"event_type\"] == \"fixation\").sum())\n",
        "    feat[\"n_saccades\"]  = int((events_df[\"event_type\"] == \"saccade\").sum())\n",
        "\n",
        "    all_features.append(feat)\n",
        "    print(f\"Processed {file}: fix={feat['n_fixations']}, sac={feat['n_saccades']}\")\n",
        "\n",
        "# Save all features to CSV\n",
        "if len(all_features) > 0:\n",
        "    all_df = pd.DataFrame(all_features)\n",
        "    all_df.to_csv(\"eye_features_168.csv\", index=False)\n",
        "    print(\"\\n✅ DONE. Saved → eye_features_168.csv\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No valid files processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "4b0905e8",
      "metadata": {
        "id": "4b0905e8",
        "outputId": "5a556595-a24e-49a6-e549-31997988a1e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulls in file column: 0\n",
            "Example values:\n",
            "['eye_data_unzipped/Recording Data/414FJ1/A1R.txt', 'eye_data_unzipped/Recording Data/141NH4/A1R.txt', 'eye_data_unzipped/Recording Data/512PM4/A1R.txt', 'eye_data_unzipped/Recording Data/721AE4/A1R.txt', 'eye_data_unzipped/Recording Data/132IM3/A1R.txt', 'eye_data_unzipped/Recording Data/512ED1/A1R.txt', 'eye_data_unzipped/Recording Data/622FD1/A1R.txt', 'eye_data_unzipped/Recording Data/721AF1/A1R.txt', 'eye_data_unzipped/Recording Data/712WA1/A1R.txt', 'eye_data_unzipped/Recording Data/132SJ1/A1R.txt', 'eye_data_unzipped/Recording Data/512LM1/A1R.txt', 'eye_data_unzipped/Recording Data/796PA3/A1R.txt', 'eye_data_unzipped/Recording Data/754TD3/A1R.txt', 'eye_data_unzipped/Recording Data/133GJ3/A1R.txt', 'eye_data_unzipped/Recording Data/112KA1/A1R.txt', 'eye_data_unzipped/Recording Data/762SL1/A1R.txt', 'eye_data_unzipped/Recording Data/423MD1/A1R.txt', 'eye_data_unzipped/Recording Data/794EM1/A1R.txt', 'eye_data_unzipped/Recording Data/131SA2/A1R.txt', 'eye_data_unzipped/Recording Data/414JM3/A1R.txt']\n",
            "Bad rows count: 0\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"eye_features_168.csv\")\n",
        "\n",
        "print(\"Nulls in file column:\", df[\"file\"].isna().sum())\n",
        "print(\"Example values:\")\n",
        "print(df[\"file\"].head(20).tolist())\n",
        "\n",
        "# Find the bad ones (no slash and no backslash)\n",
        "bad = df[df[\"file\"].astype(str).apply(lambda s: (\"/\" not in s) and (\"\\\\\" not in s))]\n",
        "print(\"Bad rows count:\", len(bad))\n",
        "print(bad[\"file\"].head(20).tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "469e72bf",
      "metadata": {
        "id": "469e72bf",
        "outputId": "03a4637f-0e0c-4e6f-90d2-486164a0b5a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              file  subject_code\n",
            "0  eye_data_unzipped/Recording Data/414FJ1/A1R.txt             1\n",
            "1  eye_data_unzipped/Recording Data/141NH4/A1R.txt             4\n",
            "2  eye_data_unzipped/Recording Data/512PM4/A1R.txt             4\n",
            "3  eye_data_unzipped/Recording Data/721AE4/A1R.txt             4\n",
            "4  eye_data_unzipped/Recording Data/132IM3/A1R.txt             3\n",
            "subject_code\n",
            "1    76\n",
            "3    69\n",
            "2    21\n",
            "4    19\n",
            "Name: count, dtype: int64\n",
            "Columns added and CSV saved successfully!\n",
            "                                              file  subject_code risk_group  \\\n",
            "0  eye_data_unzipped/Recording Data/414FJ1/A1R.txt             1  High Risk   \n",
            "1  eye_data_unzipped/Recording Data/141NH4/A1R.txt             4   Low Risk   \n",
            "2  eye_data_unzipped/Recording Data/512PM4/A1R.txt             4   Low Risk   \n",
            "3  eye_data_unzipped/Recording Data/721AE4/A1R.txt             4   Low Risk   \n",
            "4  eye_data_unzipped/Recording Data/132IM3/A1R.txt             3   Low Risk   \n",
            "\n",
            "   gender  risk_group_label  gender_label  \n",
            "0    Male                 1             1  \n",
            "1  Female                 0             0  \n",
            "2  Female                 0             0  \n",
            "3  Female                 0             0  \n",
            "4    Male                 0             1  \n",
            "risk_group\n",
            "High Risk    97\n",
            "Low Risk     88\n",
            "Name: count, dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [file, subject_code]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"eye_features_168.csv\")\n",
        "\n",
        "def get_subject_code(file_path):\n",
        "    # normalize Windows \\ to /\n",
        "    s = str(file_path).replace(\"\\\\\", \"/\")\n",
        "    folder_name = s.split(\"/\")[-2]      # e.g. \"111GM3\"\n",
        "    return int(folder_name[-1])         # -> 3\n",
        "\n",
        "df[\"subject_code\"] = df[\"file\"].apply(get_subject_code)\n",
        "\n",
        "print(df[[\"file\", \"subject_code\"]].head())\n",
        "print(df[\"subject_code\"].value_counts())\n",
        "\n",
        "\n",
        "# Apply function to create 'subject_code' column\n",
        "df['subject_code'] = df['file'].apply(get_subject_code)\n",
        "\n",
        "# Function to assign risk group\n",
        "def assign_risk(code):\n",
        "    if code in [1, 2]:\n",
        "        return 'High Risk'\n",
        "    elif code in [3, 4]:\n",
        "        return 'Low Risk'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Function to assign gender\n",
        "def assign_gender(code):\n",
        "    if code in [1, 3]:\n",
        "        return 'Male'\n",
        "    elif code in [2, 4]:\n",
        "        return 'Female'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Apply functions to create 'risk_group' and 'gender' columns\n",
        "df['risk_group'] = df['subject_code'].apply(assign_risk)\n",
        "df['gender'] = df['subject_code'].apply(assign_gender)\n",
        "\n",
        "# Convert categorical to numeric for machine learning\n",
        "df['risk_group_label'] = df['risk_group'].map({'High Risk': 1, 'Low Risk': 0})\n",
        "df['gender_label'] = df['gender'].map({'Male': 1, 'Female': 0})\n",
        "\n",
        "# Save updated DataFrame to a new CSV\n",
        "df.to_csv(\"eye_features_with_labels.csv\", index=False)\n",
        "\n",
        "print(\"Columns added and CSV saved successfully!\")\n",
        "print(df[['file','subject_code','risk_group','gender','risk_group_label','gender_label']].head())\n",
        "print(df['risk_group'].value_counts(dropna=False))\n",
        "print(df[df['risk_group'].isin(['Unknown'])][['file','subject_code']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "48dca9e6",
      "metadata": {
        "id": "48dca9e6",
        "outputId": "ffffdfeb-001b-40bb-efe0-d03a1012ed9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "risk_group\n",
            "High Risk    97\n",
            "Low Risk     88\n",
            "Name: count, dtype: int64\n",
            "gender\n",
            "Male      145\n",
            "Female     40\n",
            "Name: count, dtype: int64\n",
            "Unknown rows: 0 0\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"eye_features_with_labels.csv\")\n",
        "print(df['risk_group'].value_counts(dropna=False))\n",
        "print(df['gender'].value_counts(dropna=False))\n",
        "print(\"Unknown rows:\", (df['risk_group']==\"Unknown\").sum(), (df['gender']==\"Unknown\").sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"eye_features_with_labels.csv\")  # change filename if needed\n",
        "\n",
        "print(\"Total columns:\", len(df.columns))\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(i, col)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSL9acuWQ5Pt",
        "outputId": "d0f04ae3-6e7b-4c95-cf40-dad75c61dad6"
      },
      "id": "WSL9acuWQ5Pt",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total columns: 177\n",
            "0 fix_prog_duration_mean\n",
            "1 fix_prog_duration_std\n",
            "2 fix_prog_version_x_distance_mean\n",
            "3 fix_prog_version_x_distance_std\n",
            "4 fix_prog_version_y_distance_mean\n",
            "5 fix_prog_version_y_distance_std\n",
            "6 fix_prog_vergence_x_distance_mean\n",
            "7 fix_prog_vergence_x_distance_std\n",
            "8 fix_prog_vergence_y_distance_mean\n",
            "9 fix_prog_vergence_y_distance_std\n",
            "10 fix_prog_version_x_mean_mean\n",
            "11 fix_prog_version_x_mean_std\n",
            "12 fix_prog_version_y_mean_mean\n",
            "13 fix_prog_version_y_mean_std\n",
            "14 fix_prog_vergence_x_mean_mean\n",
            "15 fix_prog_vergence_x_mean_std\n",
            "16 fix_prog_vergence_y_mean_mean\n",
            "17 fix_prog_vergence_y_mean_std\n",
            "18 fix_prog_version_x_std_mean\n",
            "19 fix_prog_version_x_std_std\n",
            "20 fix_prog_version_y_std_mean\n",
            "21 fix_prog_version_y_std_std\n",
            "22 fix_prog_vergence_x_std_mean\n",
            "23 fix_prog_vergence_x_std_std\n",
            "24 fix_prog_vergence_y_std_mean\n",
            "25 fix_prog_vergence_y_std_std\n",
            "26 fix_prog_version_x_range_mean\n",
            "27 fix_prog_version_x_range_std\n",
            "28 fix_prog_version_y_range_mean\n",
            "29 fix_prog_version_y_range_std\n",
            "30 fix_prog_vergence_x_range_mean\n",
            "31 fix_prog_vergence_x_range_std\n",
            "32 fix_prog_vergence_y_range_mean\n",
            "33 fix_prog_vergence_y_range_std\n",
            "34 fix_prog_version_x_accum_mean\n",
            "35 fix_prog_version_x_accum_std\n",
            "36 fix_prog_version_y_accum_mean\n",
            "37 fix_prog_version_y_accum_std\n",
            "38 fix_prog_vergence_x_accum_mean\n",
            "39 fix_prog_vergence_x_accum_std\n",
            "40 fix_prog_vergence_y_accum_mean\n",
            "41 fix_prog_vergence_y_accum_std\n",
            "42 fix_reg_duration_mean\n",
            "43 fix_reg_duration_std\n",
            "44 fix_reg_version_x_distance_mean\n",
            "45 fix_reg_version_x_distance_std\n",
            "46 fix_reg_version_y_distance_mean\n",
            "47 fix_reg_version_y_distance_std\n",
            "48 fix_reg_vergence_x_distance_mean\n",
            "49 fix_reg_vergence_x_distance_std\n",
            "50 fix_reg_vergence_y_distance_mean\n",
            "51 fix_reg_vergence_y_distance_std\n",
            "52 fix_reg_version_x_mean_mean\n",
            "53 fix_reg_version_x_mean_std\n",
            "54 fix_reg_version_y_mean_mean\n",
            "55 fix_reg_version_y_mean_std\n",
            "56 fix_reg_vergence_x_mean_mean\n",
            "57 fix_reg_vergence_x_mean_std\n",
            "58 fix_reg_vergence_y_mean_mean\n",
            "59 fix_reg_vergence_y_mean_std\n",
            "60 fix_reg_version_x_std_mean\n",
            "61 fix_reg_version_x_std_std\n",
            "62 fix_reg_version_y_std_mean\n",
            "63 fix_reg_version_y_std_std\n",
            "64 fix_reg_vergence_x_std_mean\n",
            "65 fix_reg_vergence_x_std_std\n",
            "66 fix_reg_vergence_y_std_mean\n",
            "67 fix_reg_vergence_y_std_std\n",
            "68 fix_reg_version_x_range_mean\n",
            "69 fix_reg_version_x_range_std\n",
            "70 fix_reg_version_y_range_mean\n",
            "71 fix_reg_version_y_range_std\n",
            "72 fix_reg_vergence_x_range_mean\n",
            "73 fix_reg_vergence_x_range_std\n",
            "74 fix_reg_vergence_y_range_mean\n",
            "75 fix_reg_vergence_y_range_std\n",
            "76 fix_reg_version_x_accum_mean\n",
            "77 fix_reg_version_x_accum_std\n",
            "78 fix_reg_version_y_accum_mean\n",
            "79 fix_reg_version_y_accum_std\n",
            "80 fix_reg_vergence_x_accum_mean\n",
            "81 fix_reg_vergence_x_accum_std\n",
            "82 fix_reg_vergence_y_accum_mean\n",
            "83 fix_reg_vergence_y_accum_std\n",
            "84 sac_prog_duration_mean\n",
            "85 sac_prog_duration_std\n",
            "86 sac_prog_version_x_distance_mean\n",
            "87 sac_prog_version_x_distance_std\n",
            "88 sac_prog_version_y_distance_mean\n",
            "89 sac_prog_version_y_distance_std\n",
            "90 sac_prog_vergence_x_distance_mean\n",
            "91 sac_prog_vergence_x_distance_std\n",
            "92 sac_prog_vergence_y_distance_mean\n",
            "93 sac_prog_vergence_y_distance_std\n",
            "94 sac_prog_version_x_mean_mean\n",
            "95 sac_prog_version_x_mean_std\n",
            "96 sac_prog_version_y_mean_mean\n",
            "97 sac_prog_version_y_mean_std\n",
            "98 sac_prog_vergence_x_mean_mean\n",
            "99 sac_prog_vergence_x_mean_std\n",
            "100 sac_prog_vergence_y_mean_mean\n",
            "101 sac_prog_vergence_y_mean_std\n",
            "102 sac_prog_version_x_std_mean\n",
            "103 sac_prog_version_x_std_std\n",
            "104 sac_prog_version_y_std_mean\n",
            "105 sac_prog_version_y_std_std\n",
            "106 sac_prog_vergence_x_std_mean\n",
            "107 sac_prog_vergence_x_std_std\n",
            "108 sac_prog_vergence_y_std_mean\n",
            "109 sac_prog_vergence_y_std_std\n",
            "110 sac_prog_version_x_range_mean\n",
            "111 sac_prog_version_x_range_std\n",
            "112 sac_prog_version_y_range_mean\n",
            "113 sac_prog_version_y_range_std\n",
            "114 sac_prog_vergence_x_range_mean\n",
            "115 sac_prog_vergence_x_range_std\n",
            "116 sac_prog_vergence_y_range_mean\n",
            "117 sac_prog_vergence_y_range_std\n",
            "118 sac_prog_version_x_accum_mean\n",
            "119 sac_prog_version_x_accum_std\n",
            "120 sac_prog_version_y_accum_mean\n",
            "121 sac_prog_version_y_accum_std\n",
            "122 sac_prog_vergence_x_accum_mean\n",
            "123 sac_prog_vergence_x_accum_std\n",
            "124 sac_prog_vergence_y_accum_mean\n",
            "125 sac_prog_vergence_y_accum_std\n",
            "126 sac_reg_duration_mean\n",
            "127 sac_reg_duration_std\n",
            "128 sac_reg_version_x_distance_mean\n",
            "129 sac_reg_version_x_distance_std\n",
            "130 sac_reg_version_y_distance_mean\n",
            "131 sac_reg_version_y_distance_std\n",
            "132 sac_reg_vergence_x_distance_mean\n",
            "133 sac_reg_vergence_x_distance_std\n",
            "134 sac_reg_vergence_y_distance_mean\n",
            "135 sac_reg_vergence_y_distance_std\n",
            "136 sac_reg_version_x_mean_mean\n",
            "137 sac_reg_version_x_mean_std\n",
            "138 sac_reg_version_y_mean_mean\n",
            "139 sac_reg_version_y_mean_std\n",
            "140 sac_reg_vergence_x_mean_mean\n",
            "141 sac_reg_vergence_x_mean_std\n",
            "142 sac_reg_vergence_y_mean_mean\n",
            "143 sac_reg_vergence_y_mean_std\n",
            "144 sac_reg_version_x_std_mean\n",
            "145 sac_reg_version_x_std_std\n",
            "146 sac_reg_version_y_std_mean\n",
            "147 sac_reg_version_y_std_std\n",
            "148 sac_reg_vergence_x_std_mean\n",
            "149 sac_reg_vergence_x_std_std\n",
            "150 sac_reg_vergence_y_std_mean\n",
            "151 sac_reg_vergence_y_std_std\n",
            "152 sac_reg_version_x_range_mean\n",
            "153 sac_reg_version_x_range_std\n",
            "154 sac_reg_version_y_range_mean\n",
            "155 sac_reg_version_y_range_std\n",
            "156 sac_reg_vergence_x_range_mean\n",
            "157 sac_reg_vergence_x_range_std\n",
            "158 sac_reg_vergence_y_range_mean\n",
            "159 sac_reg_vergence_y_range_std\n",
            "160 sac_reg_version_x_accum_mean\n",
            "161 sac_reg_version_x_accum_std\n",
            "162 sac_reg_version_y_accum_mean\n",
            "163 sac_reg_version_y_accum_std\n",
            "164 sac_reg_vergence_x_accum_mean\n",
            "165 sac_reg_vergence_x_accum_std\n",
            "166 sac_reg_vergence_y_accum_mean\n",
            "167 sac_reg_vergence_y_accum_std\n",
            "168 file\n",
            "169 sampling_rate\n",
            "170 n_fixations\n",
            "171 n_saccades\n",
            "172 subject_code\n",
            "173 risk_group\n",
            "174 gender\n",
            "175 risk_group_label\n",
            "176 gender_label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"eye_features_with_labels.csv\")\n",
        "\n",
        "# Select features and target\n",
        "EXCLUDE_COLS = {\"file\",\"risk_group\",\"gender\",\"risk_group_label\",\"gender_label\",\"subject_code\"}\n",
        "META_FEATURES = {\"sampling_rate\",\"n_fixations\",\"n_saccades\"}\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in EXCLUDE_COLS and c not in META_FEATURES]\n",
        "X = df[feature_cols].astype(float).values\n",
        "y = df[\"risk_group_label\"].astype(int).values\n",
        "\n",
        "print(f\"Dataset: {len(X)} subjects, {X.shape[1]} features\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Parameters\n",
        "n_splits = 10\n",
        "n_repeats = 10  # you can reduce for speed\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Store results\n",
        "all_accuracies = []\n",
        "all_sensitivities = []  # ← Added\n",
        "all_specificities = []  # ← Added\n",
        "last_repeat_true = []\n",
        "last_repeat_pred = []\n",
        "\n",
        "total_iterations = n_splits * n_repeats\n",
        "current_iteration = 0\n",
        "\n",
        "print(f\"\\nStarting {n_repeats} × {n_splits}-fold CV with Random Forest + GridSearch...\\n\")\n",
        "\n",
        "for repeat in range(n_repeats):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=repeat)\n",
        "\n",
        "    for train_idx, test_idx in skf.split(X, y):\n",
        "        current_iteration += 1\n",
        "        if current_iteration % 10 == 0:\n",
        "            print(f\"Progress: {current_iteration}/{total_iterations} folds ({100*current_iteration/total_iterations:.1f}%)\")\n",
        "\n",
        "        X_train_raw, X_test_raw = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train_raw)\n",
        "        X_test = scaler.transform(X_test_raw)\n",
        "\n",
        "        # GridSearchCV inside the fold\n",
        "        rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "        grid_search = GridSearchCV(\n",
        "            rf,\n",
        "            rf_param_grid,\n",
        "            cv=3,           # inner CV for hyperparameter tuning\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_rf = grid_search.best_estimator_\n",
        "\n",
        "        # Test on the outer fold\n",
        "        y_pred = best_rf.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        all_accuracies.append(acc)\n",
        "\n",
        "        # Calculate sensitivity and specificity for this fold ← Added\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Recall for positive class (LR)\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0  # Recall for negative class (HR)\n",
        "\n",
        "        all_sensitivities.append(sensitivity)\n",
        "        all_specificities.append(specificity)\n",
        "\n",
        "        # Store last repeat predictions\n",
        "        if repeat == n_repeats - 1:\n",
        "            last_repeat_true.extend(y_test)\n",
        "            last_repeat_pred.extend(y_pred)\n",
        "\n",
        "# ============================================================\n",
        "# Results\n",
        "# ============================================================\n",
        "mean_acc = np.mean(all_accuracies)\n",
        "std_acc = np.std(all_accuracies)\n",
        "mean_sens = np.mean(all_sensitivities)  # ← Added\n",
        "std_sens = np.std(all_sensitivities)    # ← Added\n",
        "mean_spec = np.mean(all_specificities)  # ← Added\n",
        "std_spec = np.std(all_specificities)    # ← Added\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESULTS: Random Forest with GridSearch\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Mean Accuracy:    {mean_acc:.4f} ± {std_acc:.4f}\")\n",
        "print(f\"Mean Sensitivity: {mean_sens:.4f} ± {std_sens:.4f}\")  # ← Added\n",
        "print(f\"Mean Specificity: {mean_spec:.4f} ± {std_spec:.4f}\")  # ← Added\n",
        "print(\"=\"*70)\n",
        "\n",
        "cm_final = confusion_matrix(last_repeat_true, last_repeat_pred)\n",
        "print(\"\\nConfusion Matrix (last repeat):\")\n",
        "print(cm_final)\n",
        "tn_final, fp_final, fn_final, tp_final = cm_final.ravel()\n",
        "print(f\"\\nTrue Negatives: {tn_final}, False Positives: {fp_final}\")\n",
        "print(f\"False Negatives: {fn_final}, True Positives: {tp_final}\")\n",
        "\n",
        "print(\"\\nClassification Report (last repeat):\")\n",
        "print(classification_report(last_repeat_true, last_repeat_pred, target_names=['HR','LR']))\n",
        "\n",
        "# ============================================================\n",
        "# Train final model on entire dataset\n",
        "# ============================================================\n",
        "scaler_final = StandardScaler()\n",
        "X_final = scaler_final.fit_transform(X)\n",
        "\n",
        "rf_final = RandomForestClassifier(random_state=42, n_jobs=-1, **grid_search.best_params_)\n",
        "rf_final.fit(X_final, y)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Final Random Forest model trained on entire dataset.\")\n",
        "print(f\"Expected performance on new data:\")\n",
        "print(f\"  Accuracy:    {mean_acc:.4f}\")\n",
        "print(f\"  Sensitivity: {mean_sens:.4f}\")\n",
        "print(f\"  Specificity: {mean_spec:.4f}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "gQv6qhFStC68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e872a5-4cfe-4f4a-daa5-e2b9f5ab1409"
      },
      "id": "gQv6qhFStC68",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 185 subjects, 168 features\n",
            "Class distribution: [88 97]\n",
            "\n",
            "Starting 10 × 10-fold CV with Random Forest + GridSearch...\n",
            "\n",
            "Progress: 10/100 folds (10.0%)\n",
            "Progress: 20/100 folds (20.0%)\n",
            "Progress: 30/100 folds (30.0%)\n",
            "Progress: 40/100 folds (40.0%)\n",
            "Progress: 50/100 folds (50.0%)\n",
            "Progress: 60/100 folds (60.0%)\n",
            "Progress: 70/100 folds (70.0%)\n",
            "Progress: 80/100 folds (80.0%)\n",
            "Progress: 90/100 folds (90.0%)\n",
            "Progress: 100/100 folds (100.0%)\n",
            "======================================================================\n",
            "RESULTS: Random Forest with GridSearch\n",
            "======================================================================\n",
            "Mean Accuracy:    0.8425 ± 0.0726\n",
            "Mean Sensitivity: 0.8144 ± 0.1102\n",
            "Mean Specificity: 0.8729 ± 0.1079\n",
            "======================================================================\n",
            "\n",
            "Confusion Matrix (last repeat):\n",
            "[[77 11]\n",
            " [19 78]]\n",
            "\n",
            "True Negatives: 77, False Positives: 11\n",
            "False Negatives: 19, True Positives: 78\n",
            "\n",
            "Classification Report (last repeat):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          HR       0.80      0.88      0.84        88\n",
            "          LR       0.88      0.80      0.84        97\n",
            "\n",
            "    accuracy                           0.84       185\n",
            "   macro avg       0.84      0.84      0.84       185\n",
            "weighted avg       0.84      0.84      0.84       185\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Final Random Forest model trained on entire dataset.\n",
            "Expected performance on new data:\n",
            "  Accuracy:    0.8425\n",
            "  Sensitivity: 0.8144\n",
            "  Specificity: 0.8729\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "# Save the model and scaler together\n",
        "joblib.dump({\n",
        "    'model': rf_final,\n",
        "    'scaler': scaler_final,\n",
        "    'feature_columns': feature_cols  # so you know the order later\n",
        "}, 'rf_final_model.pkl')\n",
        "\n",
        "# Download the file to your local machine\n",
        "files.download('rf_final_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "G5EUok3vFB6s",
        "outputId": "752c6758-6c03-43f9-dd1b-2149fd1fb598"
      },
      "id": "G5EUok3vFB6s",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_39e9f635-681b-4211-9b8f-16f58cc3169b\", \"rf_final_model.pkl\", 646455)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}